---
title: "The Psychology of Shadow AI: Why Employees Bypass Controls"
date: 2024-04-14 09:00:00 -0500
categories: [Behavioral Science, AI Governance]
tags: [Risk Culture, Shadow AI, Cognitive Load, Psychology]
pin: true
---

# Beyond the Block: Understanding the "Why" of Shadow AI

In the world of GRC, we often treat "Shadow AI"—the use of unauthorized LLMs and tools—as a disciplinary issue. We assume that if an employee bypasses a security control, they are being negligent. I argue that Shadow AI is rarely about negligence. In most cases, it is a predictable, rational response to cognitive load and systemic friction.

It is a predictable response to **Cognitive Load** and **Systemic Friction**.

## The Cognitive Friction in Practice
Human beings are biologically wired for the "Path of Least Resistance." In psychology, this is known as the **Principle of Least Effort**. When a GRC framework introduces too many clicks, long approval wait times, or complex logins, it creates friction.

When friction exceeds an employee's desire to be compliant, they don't stop working; they find a workaround. They use their personal ChatGPT account to summarize a meeting transcript because the "approved" tool is too slow. From the employee’s perspective, the risk feels abstract while the deadline is concrete.



## Why "Blocking" Doesn't Work
Standard security responses often involve technical blocks. But from a behavioral perspective, blocking without providing a viable alternative creates **Psychological Reactance**. This is the motivational state where an individual feels their freedom of choice is being threatened, leading them to "rebel" to re-establish that freedom. At that point, security doesn’t just lose compliance. It loses visibility.

In a business context, this rebellion looks like:
* Using personal devices for work tasks.
* Obfuscating data to bypass filters.
* Decreased trust in the security team.

## The Human-Centric Solution: Nudge over Block
To protect business value, we must design frameworks that account for mental processes. Instead of rigid blocks, we apply **Behavioral Economics** (Nudges):

1. **Design first:** Reduce Choice Overload: Instead of a list of 50 policies, provide one "Golden Path" for AI usage.
2. **Feedback second:** Immediate Feedback: Use helpful UI prompts that explain *why* a tool is unauthorized and immediately provide a link to the "Safe" alternative.
3. **Culture last:** Align Incentives: Reward teams that find "Safe" ways to use AI rather than just punishing those who don't.

## Challenging the Status Quo
Governance should not be a barrier; it should be the map that allows the business to go faster, safely. By understanding that security is a human behavior, Controls that ignore human behavior will always be bypassed. Controls that account for it scale.

---
**What’s next?** In my next article, I will dive into **: Nudging Compliance**, exploring how behavioral nudges can reduce ungoverned AI use by 50% without a single disciplinary meeting.
